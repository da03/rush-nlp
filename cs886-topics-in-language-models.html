---
layout: page
title: CS 886 Topics in Language Models (Fall 24)
permalink: /teaching/fall2024/cs886-topics-in-language-models/
weight: 6
---

<h3>CS 886: Topics in Language Models (Fall 24)</h3>
<p><strong>Instructor:</strong> Yuntian Deng</p>
<p><strong>Course Schedule:</strong> Wednesdays, 12:00 pm - 2:50 pm</p>
<p><strong>Location:</strong> DC 2585</p>
<p><strong>Enrollment Limit:</strong> 40 students</p>
<p><strong>Instructor Email:</strong> <a href="mailto:yuntian@uwaterloo.ca">yuntian@uwaterloo.ca</a></p>
<p><strong>Office Hours:</strong> <a href="https://calendly.com/yuntiandeng/office-hour">Schedule via Calendly</a></p>
<p style="color: red;"><strong>Note:</strong> This is a provisional version of the syllabus. Expect changes over time.</p>

<br>
<h4>Course Description</h4>
<p>This graduate seminar focuses on recent advancements in language models. In each session, students will present and discuss on recent papers in the field. The course emphasizes critical analysis, enabling students to understand the strengths, limitations, and emerging trends in language modeling.</p>

<br>
<h4>Grading</h4>
<p>All deliverables are due by 11:59pm Eastern Time on the respective due date. Late submissions will only be considered with prior approval from the instructor.</p>

<table class="table table-bordered">
  <thead>
    <tr>
      <th>Task</th>
      <th>Due Date</th>
      <th>Weight</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Class Participation</td>
      <td>Throughout the semester</td>
      <td>20%</td>
    </tr>
    <tr>
      <td>Two Presentations</td>
      <td>As assigned</td>
      <td>40%</td>
    </tr>
    <tr>
      <td>Project: Team Formation</td>
      <td>Sep 25</td>
      <td>-</td>
    </tr>
    <tr>
      <td>Project: Proposal</td>
      <td>Oct 11</td>
      <td>10%</td>
    </tr>
    <tr>
      <td>Project: Progress Report</td>
      <td>Nov 08</td>
      <td>10%</td>
    </tr>
    <tr>
      <td>Project: Presentation</td>
      <td>Nov 27</td>
      <td>10%</td>
    </tr>
    <tr>
      <td>Project: Final Report</td>
      <td>Dec 05</td>
      <td>10%</td>
    </tr>
  </tbody>
</table>

<br>
<h4>Intended Learning Outcomes</h4>
<ul>
  <li>Keep up-to-date with the current progress in language models.</li>
  <li>Critically analyze research publications.</li>
  <li>Develop and present original research ideas related to language models.</li>
</ul>

<br>
<h4>Prerequisites and Recommended Materials</h4>
<ul>
  <li>Familiarity with probabilistic theory</li>
  <li>Understanding of numeric optimization (backpropagation)</li>
  <li>Proficiency in Python, PyTorch, and tensor programming</li>
  <li>Ability to independently read and understand research papers for active participation in discussions</li>
</ul>
<p>Below is a series of exercises from my PhD advisor Professor <a href="https://rush-nlp.com/">Alexander Rush</a> that I highly recommend:</p>
<ul>
  <li><a href="https://github.com/srush/Tensor-Puzzles">Tensor Puzzles</a></li>
  <li><a href="https://github.com/srush/gpu-puzzles">GPU Puzzles</a></li>
  <li><a href="https://github.com/srush/Autodiff-Puzzles">Autodiff Puzzles</a></li>
  <li><a href="https://github.com/srush/Transformer-Puzzles">Transformer Puzzles</a></li>
  <li><a href="https://github.com/srush/GPTworld">GPT World</a></li>
  <li><a href="https://github.com/srush/LLM-Training-Puzzles">LLM Training Puzzles</a></li>
</ul>

<br>
<h4>Coursework Overview</h4>
<p><strong>Presentations:</strong> Students will present one or more papers during the course. Presentations are crucial and should not be missed, as they significantly contribute to the class dynamics. If unforeseen circumstances arise, inform the instructor as soon as possible.</p>

<p><strong>Final Project:</strong> Students will work on a final project that allows them to explore a topic related to language models in depth. Projects can be done individually or in groups of up to 3 members. Groups larger than this range require a justification and are subject to instructor approval.</p>

<ul>
  <li><strong>Original Contribution:</strong> The primary aim of the project is to make an original contribution to the field. Ideally, the project should be of a quality that could potentially lead to a publication. This could involve, but is not limited to:
    <ul>
      <li><strong>Novel Research:</strong> Developing new methods, models, or insights in the field of language models.</li>
      <li><strong>Reproducibility Test:</strong> Attempting to replicate and possibly extend the results from a published paper, providing detailed analysis and commentary on the findings.</li>
      <li><strong>Negative Results:</strong> Investigating a hypothesis or method that did not yield expected results, with thorough documentation of the process and analysis of why it failed.</li>
      <li><strong>Survey:</strong> Conducting a comprehensive and critical survey of a specific area within language models, identifying gaps or trends that could inform future research. Even for surveys, the work should go beyond summarization to include thoughtful analysis and synthesis of the literature.</li>
    </ul>
  </li>
  <li><strong>Project Timeline:</strong>
    <ul>
      <li><strong>Team Formation:</strong> By Sep 25, students form teams of up to three members and sign up at <a href="https://bit.ly/cs886signup">bit.ly/cs886signup</a> (Sheet 2).</li>
      <li><strong>Proposal Submission:</strong> By Oct 11, students submit a project proposal outlining their research question, hypothesis, and the planned approach. Submit your proposal at <a href="https://openreview.net/group?id=UWaterloo.ca/Fall_2024/CS886" target="_blank">openreview.net/group?id=UWaterloo.ca/Fall_2024/CS886</a> (make sure to register an OpenReview account using your Waterloo email).  Be sure to submit both the <strong>PDF of the proposal</strong> and the abstract.</li>
      <li><strong>Progress Report:</strong> By Nov 8, students submit a progress report detailing the work completed so far, challenges encountered, and any adjustments to the initial plan. Submit your progress report by updating your OpenReview submission.</li>
      <li><strong>Final Presentation:</strong> On Nov 27, students present their project findings to the class, providing a clear overview of their approach, results, and conclusions.</li>
       <li><strong>Final Report:</strong> By Dec 5, students submit a final project report in a format similar to a machine learning conference paper. The report should follow the <a href="https://icml.cc/Conferences/2024/AuthorInstructions" target="_blank">ICML template format</a>, and include the motivation, research question, hypothesis, approach, results, and discussion. Submit your final report by updating your OpenReview submission.</li>
    </ul>
  </li>
</ul>

<p>Participation in every class is expected since discussion is crucial to the seminar format.</p>

<h4>Syllabus and Presentation Sign-Up</h4>

<h5>Sign-up Instructions</h5>
<ul>
  <li>Sign up for <strong>two presentation slots</strong> at <a href="https://bit.ly/cs886signup">bit.ly/cs886signup</a> (Sheet 1) to receive full credit.</li>
  <li>Each section allows up to three students to sign up.</li>
  <li>Use the embedded Google Sheet below (or the link above) to sign up for presentations by adding a comment with your name in the desired cell (only the first comment in each cell will be considered valid).</li>
</ul>


<h5>Presentation Preparation Guidelines</h5>
<ul>
<li><strong>Time Management:</strong> Each group has 60 minutes (discussions included). Please make sure to stay within this time limit.</li>
<li><strong>Before Presentation:</strong> Before your presentation, ideally on the Monday before your slot, please schedule a meeting with the instructor using <a href="https://calendly.com/yuntiandeng/one-on-one">this link</a>.</li>
<li><strong>Participation:</strong> Ensure that every member of the group participates in the presentation. During the presentation, please clearly state your name so each presenter can be marked individually. You are free to choose your presentation format (whether focusing on depth or breadth), but make sure to effectively <strong>teach the audience about the topic</strong>. Expect questions and discussions during the presentation, and be prepared to engage with the class.</li>
<li><strong>Collaboration:</strong> You can find your teammates' contact information in the shared email thread. Please collaborate and decide on the paper(s) you would like to present. While you are encouraged to choose from the list provided, you are also welcome to select other relevant papers as long as they align with the course topics.</li>
<li><strong>Slides:</strong> Please work together to create a cohesive set of slides. Collaboration-friendly platforms such as Google Slides or Overleaf (with LaTeX Beamer) are recommended, but feel free to use any tool you're comfortable with. Please meet offline or virtually to ensure smooth coordination and preparation. If you are willing to share your slides, please upload them <a href="https://bit.ly/cs886slides">here</a>.</li>
</ul>


<h5>Presentation Grading Rubric</h5>
<p>The presentations will be graded on an individual basis (per presenter) according to the following criteria:</p>

<table class="table table-bordered">
  <thead>
    <tr>
      <th>Criteria</th>
      <th>4 - Excellent</th>
      <th>3 - Good</th>
      <th>2 - Satisfactory</th>
      <th>1 - Needs Improvement</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Relevance to Topic</strong></td>
      <td>Fully adheres to the assigned topic and effectively teaches the audience about the topic.</td>
      <td>Mostly adheres to the topic with minor tangents; still effectively teaches core aspects.</td>
      <td>Partially adheres to the topic but misses key elements or strays too far.</td>
      <td>Significantly deviates from the assigned topic, does not teach the relevant material effectively.</td>
    </tr>
    <tr>
      <td><strong>Content</strong></td>
      <td>Thorough understanding, covering key points with insights and analysis.</td>
      <td>Good understanding, covers most key points.</td>
      <td>Basic understanding, covers some key points but misses important points.</td>
      <td>Limited understanding or significant gaps in coverage of key points.</td>
    </tr>
    <tr>
      <td><strong>Clarity, Organization, and Visuals</strong></td>
      <td>Clear and well-structured presentation with effective use of visuals that support the content.</td>
      <td>Mostly clear; could improve in structure or visual support.</td>
      <td>Somewhat unclear or disorganized.</td>
      <td>Unclear, disorganized, or difficult to follow.</td>
    </tr>
    <tr>
      <td><strong>Engagement and Interaction</strong></td>
      <td>Actively engages the audience, encourages questions and discussion, responds thoughtfully.</td>
      <td>Engages the audience but doesn't fully encourage interaction; handles questions fairly well.</td>
      <td>Limited engagement with the audience, weak handling of questions or discussions.</td>
      <td>No meaningful audience engagement; struggles to respond to questions or avoid interaction.</td>
    </tr>
    <tr>
      <td><strong>Collaboration</strong></td>
      <td>Seamless teamwork with all members contributing equally; smooth transitions between presenters.</td>
      <td>Good teamwork but with some minor imbalances in contribution or transitions between presenters.</td>
      <td>Uneven contribution from members; transitions are awkward or lacking in coordination.</td>
      <td>Poor teamwork; significant imbalance in contributions or disjointed transitions between presenters.</td>
    </tr>
    <tr>
      <td><strong>Timeliness</strong></td>
      <td>Stays within the allocated time and pacing is excellent throughout.</td>
      <td>Mostly adheres to the time limit but with minor deviations; pacing is generally good.</td>
      <td>Significantly over or under the time limit; pacing is inconsistent.</td>
      <td>Fails to adhere to the time limit; pacing is problematic throughout.</td>
    </tr>
  </tbody>
</table>

<h6>Grading Breakdown</h6>
<ul>
  <li><strong>Relevance to Topic:</strong> 20%</li>
  <li><strong>Content Depth:</strong> 30%</li>
  <li><strong>Clarity & Visuals:</strong> 20%</li>
  <li><strong>Engagement & Interaction:</strong> 15%</li>
  <li><strong>Collaboration:</strong> 10%</li>
  <li><strong>Timeliness:</strong> 5%</li>
</ul>

<p><strong>Note:</strong>  Make sure that all members contribute and that each presenter introduces themselves clearly at the start of their part.</p>


<!--h5>Google Sheet Sign-up:</h5>
<div style="text-align: center;">
<iframe width="600" height="700" src="https://docs.google.com/spreadsheets/d/1aCSNww2wIqajGpZOvhSvcudg5wBhIqdzR4ZiJzz282s/edit?usp=sharing?widget=true&amp;single=true&amp;"></iframe>
</div-->


<!-- Syllabus Table -->
<table class="table table-striped">
  <thead>
    <tr>
      <th style="width:10%">Date</th>
      <th style="width:10%">Section</th>
      <th style="width:20%">Section Topic</th>
      <th>Recommended Papers</th>
      <th style="width:15%">Presenters</th>
    </tr>
  </thead>
  <tbody>

    <!-- September 4 -->
    <tr>
      <td><strong>Sep 4</strong></td>
      <td><strong>1</strong></td>
      <td><strong>Introduction Lecture</strong></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td></td>
      <td><strong>2</strong></td>
      <td><strong>Course Logistics</strong></td>
      <td></td>
      <td></td>
    </tr>

    <!-- September 11 -->
    <tr>
      <td><strong>Sep 11</strong></td>
      <td><strong>3</strong></td>
      <td><strong>Seq2Seq Learning</strong></td>
      <td>
        <ul>
          <li><a href="https://arxiv.org/abs/1301.3781">Efficient Estimation of Word Representations in Vector Space</a></li>
          <li><a href="https://arxiv.org/abs/1409.0473">Neural Machine Translation by Jointly Learning to Align and Translate</a></li>
          <li><a href="https://arxiv.org/abs/1508.07909">Neural Machine Translation of Rare Words with Subword Units</a></li>
          <li><a href="https://arxiv.org/abs/1603.06393">Incorporating Copying Mechanism in Sequence-to-Sequence Learning</a></li>
          <li><a href="https://arxiv.org/abs/1506.03134">Pointer Networks</a></li>
          <li><a href="https://arxiv.org/abs/1502.03044">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</a></li>
          <li><a href="https://aclanthology.org/D14-1162/">GloVe: Global Vectors for Word Representation</a></li>
          <li><a href="https://arxiv.org/abs/1409.3215">Sequence to Sequence Learning with Neural Networks</a></li>
          <li><a href="https://arxiv.org/abs/1103.0398">Natural Language Processing (almost) from Scratch</a></li>
          <li><a href="https://arxiv.org/abs/1508.04025">Effective Approaches to Attention-based Neural Machine Translation</a></li>
          <li><a href="https://arxiv.org/abs/1508.01211">Listen, Attend and Spell</a></li>
        </ul>
      </td>
      <td>Gurjot Singh<br>Delara Forghani<br>Luke Rivard</td>
    </tr>
    <tr>
      <td></td>
      <td><strong>4</strong></td>
      <td><strong>Optimization</strong></td>
      <td>
        <ul>
          <li><a href="https://arxiv.org/abs/1412.6980">Adam: A Method for Stochastic Optimization</a></li>
          <li><a href="https://arxiv.org/abs/1711.05101">Decoupled Weight Decay Regularization</a></li>
          <li><a href="https://arxiv.org/abs/1511.06732">Sequence Level Training with Recurrent Neural Networks</a></li>
          <li><a href="https://arxiv.org/abs/1610.09038">Professor Forcing: A New Algorithm for Training Recurrent Networks</a></li>
          <li><a href="https://arxiv.org/abs/1803.03635">The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks</a></li>
          <li><a href="https://arxiv.org/abs/1606.02960">Sequence-to-Sequence Learning as Beam-Search Optimization</a></li>
          <li><a href="https://arxiv.org/abs/1606.04474">Learning to learn by gradient descent by gradient descent</a></li>
          <li><a href="https://arxiv.org/abs/1611.03530">Understanding deep learning requires rethinking generalization</a></li>
          <li><a href="https://arxiv.org/abs/2402.06184">The boundary of neural network trainability is fractal</a> (<a href="https://x.com/jaschasd/status/1756930242965606582">visualization</a>)</li>
          <li><a href="https://arxiv.org/abs/1803.00885">Essentially No Barriers in Neural Network Energy Landscape</a></li>
          <li><a href="https://arxiv.org/abs/2310.13807">Learning to (Learn at Test Time)</a></li>
        </ul>
      </td>
      <td>Songcheng Cai<br>Amber Wang<br>Ruoxi Ning</td>
    </tr>

    <!-- September 18 -->
    <tr>
      <td><strong>Sep 18</strong></td>
      <td><strong>5</strong></td>
      <td><strong>Variational Autoencoders</strong></td>
      <td>
        <ul>
          <li><a href="https://arxiv.org/abs/1312.6114">Auto-Encoding Variational Bayes</a></li>
          <li><a href="https://arxiv.org/abs/1711.00937">Neural Discrete Representation Learning</a></li>
          <li><a href="https://arxiv.org/abs/1511.06349">Generating Sentences from a Continuous Space</a></li>
          <li><a href="https://arxiv.org/abs/1703.00955">Toward Controlled Generation of Text</a></li>
          <li><a href="https://arxiv.org/abs/1606.04155">Rationalizing Neural Predictions</a></li>
          <li><a href="https://arxiv.org/abs/1705.09655">Style Transfer from Non-Parallel Text by Cross-Alignment</a></li>
          <li><a href="https://arxiv.org/abs/2107.03312">SoundStream: An End-to-End Neural Audio Codec</a></li>
          <li><a href="https://arxiv.org/abs/1711.10433">Parallel WaveNet: Fast High-Fidelity Speech Synthesis</a></li>
          <li><a href="https://arxiv.org/abs/1802.02550">Semi-Amortized Variational Autoencoders</a></li>
          <li><a href="https://arxiv.org/abs/1807.03756">Latent Alignment and Variational Attention</a></li>
          <li><a href="https://arxiv.org/abs/1904.03746">Unsupervised Recurrent Neural Network Grammars</a></li>
          <li><a href="https://arxiv.org/abs/1611.01144">Categorical Reparameterization with Gumbel-Softmax</a></li>
          <li><a href="https://arxiv.org/abs/1611.00712">The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables</a></li>
          <li><a href="https://arxiv.org/abs/1901.05534">Lagging Inference Networks and Posterior Collapse in Variational Autoencoders</a></li>
          <li><a href="https://arxiv.org/abs/1706.00550">On Unifying Deep Generative Models</a></li>
          <li><a href="https://arxiv.org/abs/1603.06318">Harnessing Deep Neural Networks with Logic Rules</a></li>
          <li><a href="https://arxiv.org/abs/2006.11239">Denoising Diffusion Probabilistic Models</a></li>
        </ul>
      </td>
      <td>Achint Soni<br>Andy Zheng<br>Wentao Zhang</td>
    </tr>
    <tr>
      <td></td>
      <td><strong>6</strong></td>
      <td><strong>Knowledge Distillation</strong></td>
      <td>
        <ul>
          <li><a href="https://arxiv.org/abs/1503.02531">Distilling the Knowledge in a Neural Network</a></li>
          <li><a href="https://arxiv.org/abs/1606.07947">Sequence-Level Knowledge Distillation</a></li>
          <li><a href="https://arxiv.org/abs/1910.12366">Thieves on Sesame Street! Model Extraction of BERT-based APIs</a></li>
          <li><a href="https://arxiv.org/abs/2209.15189">Learning by Distilling Context</a></li>
          <li><a href="https://arxiv.org/abs/2311.01460">Implicit Chain of Thought Reasoning via Knowledge Distillation</a></li>
          <li><a href="https://arxiv.org/abs/2110.07178">Symbolic Knowledge Distillation: from General Language Models to Commonsense Models</a></li>
          <li><a href="https://arxiv.org/abs/2305.16635">Impossible Distillation: from Low-Quality Model to High-Quality Dataset &amp; Model for Summarization and Paraphrasing</a></li>
          <li><a href="https://arxiv.org/abs/2403.13780">Information-Theoretic Distillation for Reference-less Summarization</a></li>
        </ul>
      </td>
      <td>Hala Sheta<br>Peter Pan<br>Gurjot Singh</td>
    </tr>

    <!-- September 25 -->
    <tr>
      <td><strong>Sep 25</strong></td>
      <td><strong>7</strong></td>
      <td><strong>Transformer</strong></td>
      <td>
        <ul>
          <li><a href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a></li>
          <li><a href="https://arxiv.org/abs/1901.02860">Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</a></li>
          <li><a href="https://arxiv.org/abs/1908.06938">Encoder-Agnostic Adaptation for Conditional Language Generation</a></li>
          <li><a href="https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a></li>
          <li><a href="https://arxiv.org/abs/1607.06450">Layer Normalization</a></li>
          <li><a href="https://arxiv.org/abs/1512.03385">Deep Residual Learning for Image Recognition</a></li>
          <li><a href="https://arxiv.org/abs/1608.05859">Using the Output Embedding to Improve Language Models</a></li>
          <li><a href="https://arxiv.org/abs/2403.06634">Stealing Part of a Production Language Model</a></li>
          <li><a href="https://arxiv.org/abs/2403.09539">Logits of API-Protected LLMs Leak Proprietary Information</a></li>
          <li><a href="https://arxiv.org/abs/1603.06393">Incorporating Copying Mechanism in Sequence-to-Sequence Learning</a> (also listed under Seq2Seq)</li>
          <li><a href="https://arxiv.org/abs/1506.03134">Pointer Networks</a> (also listed under Seq2Seq)</li>
        </ul>
      </td>
      <td>Anubhav Gupta<br>Xinxin Yu<br>Shakiba Amirshahi</td>
    </tr>
    <tr>
      <td></td>
      <td><strong>8</strong></td>
      <td><strong>Pretraining</strong></td>
      <td>
        <ul>
          <li><a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></li>
          <li><a href="https://arxiv.org/abs/1802.05365">Deep contextualized word representations</a></li>
          <li><a href="https://arxiv.org/abs/1910.10683">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a></li>
          <li><a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Language Models are Unsupervised Multitask Learners</a></li>
          <li><a href="https://arxiv.org/abs/2204.02311">PaLM: Scaling Language Modeling with Pathways</a></li>
          <li><a href="https://arxiv.org/abs/2407.21783">The Llama 3 Herd of Models</a></li>
          <li><a href="https://arxiv.org/abs/2205.01068">OPT: Open Pre-trained Transformer Language Models</a></li>
          <li><a href="https://arxiv.org/abs/2211.05100">BLOOM: A 176B-Parameter Open-Access Multilingual Language Model</a></li>
          <li><a href="https://arxiv.org/abs/1910.13461">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</a></li>
          <li><a href="https://arxiv.org/abs/1906.08237">XLNet: Generalized Autoregressive Pretraining for Language Understanding</a></li>
          <li><a href="https://arxiv.org/abs/2003.10555">ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators</a></li>
        </ul>
      </td>
      <td>Mingkun Ni<br>Yuntian Deng</td>
    </tr>

    <!-- October 2 -->
    <tr>
      <td><strong>Oct 2</strong></td>
      <td><strong>9</strong></td>
      <td><strong>Non-Autoregressive Models</strong></td>
      <td>
        <ul>
          <li><a href="https://arxiv.org/abs/1711.02281">Non-Autoregressive Neural Machine Translation</a></li>
          <li><a href="https://arxiv.org/abs/1802.06901">Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement</a></li>
          <li><a href="https://arxiv.org/abs/1904.09324">Mask-Predict: Parallel Decoding of Conditional Masked Language Models</a></li>
          <li><a href="https://arxiv.org/abs/2006.10369">Deep Encoder, Shallow Decoder: Reevaluating Non-autoregressive Machine Translation</a></li>
          <li><a href="https://arxiv.org/abs/1911.02727">Understanding Knowledge Distillation in Non-autoregressive Machine Translation</a></li>
          <li><a href="https://arxiv.org/abs/1909.02480">FlowSeq: Non-Autoregressive Conditional Sequence Generation with Generative Flow</a></li>
          <li><a href="https://arxiv.org/abs/2006.01112">Cascaded Text Generation with Markov Transformers</a></li>
          <li><a href="https://arxiv.org/abs/2004.11714">Residual Energy-Based Models for Text Generation</a></li>
        </ul>
      </td>
      <td>Qianqiu Zhang<br>Peiyi Zheng<br>Hussein Barakat</td>
    </tr>
    <tr>
      <td></td>
      <td><strong>10</strong></td>
      <td><strong>Evaluation 1</strong></td>
      <td>
        <ul>
          <li><a href="https://arxiv.org/abs/1804.07461">GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding</a></li>
          <li><a href="https://arxiv.org/abs/1502.05698">Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks</a></li>
          <li><a href="https://arxiv.org/abs/1804.06028">ListOps: A Diagnostic Dataset for Latent Tree Learning</a></li>
          <li><a href="https://arxiv.org/abs/1711.00350">Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks</a></li>
          <li><a href="https://arxiv.org/abs/1910.14599">Adversarial NLI: A New Benchmark for Natural Language Understanding</a></li>
          <li><a href="https://arxiv.org/abs/2210.08444">Model Criticism for Long-Form Text Generation</a></li>
          <li><a href="https://arxiv.org/abs/2102.01454">MAUVE: Measuring the Gap Between Neural Text and Human Text using Divergence Frontiers</a></li>
        </ul>
      </td>
      <td>Mingkun Ni<br>Ruotian Wu<br>Luke Rivard</td>
    </tr>

    <!-- October 9 -->
    <tr>
      <td><strong>Oct 9</strong></td>
      <td><strong>11</strong></td>
      <td><strong>Interpretability</strong></td>
      <td>
        <ul>
          <li><a href="https://arxiv.org/abs/1506.02078">Visualizing and Understanding Recurrent Networks</a></li>
          <li><a href="https://arxiv.org/abs/1503.02406">Deep Learning and the Information Bottleneck Principle</a></li>
          <li><a href="https://arxiv.org/abs/1704.03471">What do Neural Machine Translation Models Learn about Morphology?</a></li>
          <li><a href="https://arxiv.org/abs/1703.04730">Understanding Black-box Predictions via Influence Functions</a></li>
          <li><a href="https://arxiv.org/abs/1909.03368">Designing and Interpreting Probes with Control Tasks</a></li>
          <li><a href="https://arxiv.org/pdf/2106.00737">Implicit Representations of Meaning in Neural Language Models</a></li>
          <li><a href="https://aclanthology.org/N19-1357/">Attention is not Explanation</a></li>
          <li><a href="https://arxiv.org/abs/1908.04626">Attention is not not Explanation</a></li>
          <li><a href="https://arxiv.org/abs/2109.06387">Rationales for Sequential Predictions</a></li>
          <li><a href="https://arxiv.org/abs/2306.12672">From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought</a></li>
          <li><a href="https://arxiv.org/abs/2210.13382">Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task</a></li>
          <li><a href="https://transformer-circuits.pub/2024/scaling-monosemanticity/">Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet</a></li>
          <li><a href="https://arxiv.org/abs/2406.04093">Scaling and evaluating sparse autoencoders</a></li>
        </ul>
      </td>
      <td>Tien Dat Nguyen<br>Anudeep Das</td>
    </tr>
    <tr>
      <td></td>
      <td><strong>12</strong></td>
      <td><strong>Mechanistic Interpretability</strong></td>
      <td>
        <ul>
          <li><a href="https://physics.allen-zhu.com/">Physics of Language Models</a></li>
          <li><a href="https://arxiv.org/abs/2406.03445v1">Pre-trained Large Language Models Use Fourier Features to Compute Addition</a></li>
          <li><a href="https://arxiv.org/abs/2407.11421v1">States Hidden in Hidden States: LLMs Emerge Discrete State Representations Implicitly</a></li>
          <li><a href="https://arxiv.org/abs/2405.15071v2">Grokked Transformers are Implicit Reasoners: A Mechanistic Journey to the Edge of Generalization</a></li>
          <li><a href="https://arxiv.org/abs/2211.07349">Finding Skill Neurons in Pre-trained Transformer-based Language Models</a></li>
          <li><a href="https://arxiv.org/abs/2201.02177">Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets</a></li>
        </ul>
      </td>
      <td>Qianqiu Zhang<br>Peiyi Zheng<br>Chang Liu</td>
    </tr>

    <!-- Reading Week -->
    <!--tr>
      <td><strong>Oct 12-20</strong></td>
      <td colspan="7"><strong>Reading Week - No Class</strong></td> (if have space, cover data, influence function, emerging communication)
    </tr-->

    <!-- October 23 -->
    <tr>
      <td><strong>Oct 23</strong></td>
      <td><strong>13</strong></td>
      <td><strong>Knowledge Editing</strong></td>
      <td>
        <ul>
          <li><a href="https://arxiv.org/abs/2202.05262">Locating and Editing Factual Associations in GPT</a></li>
          <li><a href="https://arxiv.org/abs/2306.03341">Inference-Time Intervention: Eliciting Truthful Answers from a Language Model</a></li>
          <li><a href="https://arxiv.org/abs/2104.08696">Knowledge neurons in pretrained transformers</a></li>
          <li><a href="https://arxiv.org/abs/2304.00740">Inspecting and Editing Knowledge Representations in Language Models</a></li>
        </ul>
      </td>
      <td>Arthur Chen<br>Benjamin Schneider<br>Tien Dat Nguyen</td>
    </tr>
    <tr>
      <td></td>
      <td><strong>14</strong></td>
      <td><strong>Adversarial Robustness</strong></td>
      <td>
        <ul>
          <li><a href="https://arxiv.org/abs/1802.08195">Adversarial Examples that Fool both Computer Vision and Time-Limited Humans</a></li>
          <li><a href="https://arxiv.org/abs/1312.6199">Intriguing properties of neural networks</a></li>
          <li><a href="https://arxiv.org/abs/1412.6572">Explaining and Harnessing Adversarial Examples</a></li>
          <li><a href="https://arxiv.org/abs/2307.15043">Universal and Transferable Adversarial Attacks on Aligned Language Models</a></li>
          <li><a href="https://arxiv.org/abs/2404.13208v1">The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions</a></li>
          <li><a href="https://arxiv.org/abs/1905.02175">Adversarial Examples Are Not Bugs, They Are Features</a></li>
          <li><a href="https://arxiv.org/abs/1908.07125">Universal Adversarial Triggers for Attacking and Analyzing NLP</a></li>
          <li><a href="https://arxiv.org/abs/1707.07328">Adversarial Examples for Evaluating Reading Comprehension Systems</a></li>
          <li><a href="https://arxiv.org/abs/2005.05909">Textattack: A framework for adversarial attacks, data augmentation, and adversarial training in nlp</a></li>
        </ul>
      </td>
      <td>Amir David<br>Wentao Zhang</td>
    </tr>

    <!-- October 30 -->
    <tr>
      <td><strong>Oct 30</strong></td>
      <td><strong>15</strong></td>
      <td><strong>Prompting</strong></td>
      <td>
        <ul>
          <li><a href="https://arxiv.org/abs/2410.05603">Everything Everywhere All at Once: LLMs can In-Context Learn Multiple Tasks in Superposition</a></li>
          <li><a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a></li>
          <li><a href="https://arxiv.org/abs/2201.11903">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a></li>
          <li><a href="https://arxiv.org/abs/2210.03629">ReAct: Synergizing Reasoning and Acting in Language Models</a></li>
          <li><a href="https://arxiv.org/abs/2305.10601">Tree of Thoughts: Deliberate Problem Solving with Large Language Models</a></li>
          <li><a href="https://arxiv.org/abs/2310.14034">Tree Prompting: Efficient Task Adaptation without Fine-Tuning</a></li>
          <li><a href="https://arxiv.org/abs/2305.16291">Voyager: An Open-Ended Embodied Agent with Large Language Models</a></li>
          <li><a href="https://arxiv.org/abs/2211.11559">Visual Programming: Compositional visual reasoning without training</a></li>
          <li><a href="https://arxiv.org/abs/2112.00114">Show Your Work: Scratchpads for Intermediate Computation with Language Models</a></li>
          <li><a href="https://arxiv.org/abs/2310.11324">Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting</a></li>
          <li><a href="https://arxiv.org/abs/2211.12588">Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks</a></li>
          <li><a href="https://arxiv.org/abs/2406.09403">Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models</a></li>
          <li><a href="https://arxiv.org/abs/2005.11401">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</a></li>
          <li><a href="https://arxiv.org/abs/2309.03882">Large Language Models Are Not Robust Multiple Choice Selectors</a></li>
          <li><a href="https://aclanthology.org/2023.emnlp-demo.27/">MiniChain: A Small Library for Coding with Large Language Models</a></li>
        </ul>
      </td>
      <td>Hala Sheta<br>Anubhav Gupta<br>Hussein Barakat</td>
    </tr>
    <tr>
      <td></td>
      <td><strong>16</strong></td>
      <td><strong>Prompt Engineering</strong></td>
      <td>
        <ul>
          <li><a href="https://arxiv.org/abs/2211.01910">Large Language Models Are Human-Level Prompt Engineers</a></li>
          <li><a href="https://arxiv.org/abs/2010.15980">AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts</a></li>
          <li><a href="https://arxiv.org/abs/2406.07496">TextGrad: Automatic "Differentiation" via Text</a></li>
          <li><a href="https://arxiv.org/abs/2310.03714">DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines</a></li>
          <li><a href="https://openreview.net/forum?id=GvMuB-YsiK6">Explaining Patterns in Data with Language Models via Interpretable Autoprompting</a></li>
          <li><a href="https://arxiv.org/abs/2201.12323">Describing Differences between Text Distributions with Natural Language</a></li>
        </ul>
      </td>
      <td>Han Zhou<br>Delara Forghani<br>Shakiba Amirshahi</td>
    </tr>

    <!-- November 6 -->
    <tr>
      <td><strong>Nov 6</strong></td>
      <td><strong>17</strong></td>
      <td><strong>Alignment</strong></td>
      <td>
        <ul>
          <li><a href="https://arxiv.org/abs/2203.02155">Training language models to follow instructions with human feedback</a></li>
          <li><a href="https://arxiv.org/abs/2312.01552">The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning</a></li>
          <li><a href="https://arxiv.org/abs/2009.01325">Learning to summarize from human feedback</a></li>
          <li><a href="https://arxiv.org/abs/2305.18290">Direct Preference Optimization: Your Language Model is Secretly a Reward Model</a></li>
          <li><a href="https://arxiv.org/abs/2212.10560">Self-Instruct: Aligning Language Models with Self-Generated Instructions</a></li>
          <li><a href="https://arxiv.org/abs/2406.08464">Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing</a></li>
          <li><a href="https://arxiv.org/abs/2309.05653">MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning</a></li>
        </ul>
      </td>
      <td>Amir David<br>Anudeep Das</td>
    </tr>
    <tr>
      <td></td>
      <td><strong>18</strong></td>
      <td><strong>Evaluation 2</strong></td>
      <td>
        <ul>
          <li><a href="https://arxiv.org/abs/2306.05685">Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</a></li>
          <li><a href="https://arxiv.org/abs/2107.03374">Evaluating Large Language Models Trained on Code</a></li>
          <li><a href="https://arxiv.org/abs/2310.06770">SWE-bench: Can Language Models Resolve Real-World GitHub Issues?</a></li>
          <li><a href="https://arxiv.org/abs/2009.03300">Measuring Massive Multitask Language Understanding</a></li>
          <li><a href="https://arxiv.org/abs/2303.11897">TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation with Question Answering</a></li>
          <li><a href="https://arxiv.org/abs/2403.13787">RewardBench: Evaluating Reward Models for Language Modeling</a></li>
          <li><a href="https://arxiv.org/abs/2311.12022">GPQA: A Graduate-Level Google-Proof Q&A Benchmark</a></li>
          <li><a href="https://arxiv.org/abs/2406.01574">MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark</a></li>
          <li><a href="https://arxiv.org/abs/2305.14251">FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation</a></li>
          <li><a href="https://arxiv.org/abs/2407.06004">Perceptions to Beliefs: Exploring Precursory Inferences for Theory of Mind in Large Language Models</a></li>
          <li><a href="https://arxiv.org/abs/2407.18370">Trust or Escalate: LLM Judges with Provable Guarantees for Human Agreement</a></li>
          <li><a href="https://arxiv.org/abs/2406.04770">WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild</a></li>
          <li><a href="https://arxiv.org/abs/2311.16502">MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI</a></li>
          <li><a href="https://arxiv.org/abs/2404.12390">BLINK: Multimodal Large Language Models Can See but Not Perceive</a></li>
        </ul>
      </td>
      <td>Peter Pan<br>Arthur Chen<br>Xinxin Yu</td>
    </tr>

    <!-- November 13 -->
    <tr>
      <td><strong>Nov 13</strong></td>
      <td><strong>19</strong></td>
      <td><strong>Training Scalability</strong></td>
      <td>
        <ul>
          <li><a href="https://arxiv.org/abs/2106.09685">LoRA: Low-Rank Adaptation of Large Language Models</a></li>
          <li><a href="https://dl.acm.org/doi/10.1145/3394486.3406703">DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters</a></li>
          <li><a href="https://arxiv.org/abs/2402.08268">World Model on Million-Length Video And Language With Blockwise RingAttention</a></li>
          <li><a href="https://arxiv.org/abs/2204.06745">GPT-NeoX-20B: An Open-Source Autoregressive Language Model</a></li>
          <li><a href="https://arxiv.org/abs/1909.08053v4">Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</a></li>
          <li><a href="https://arxiv.org/abs/2203.15556">Training Compute-Optimal Large Language Models</a></li>
          <li><a href="https://arxiv.org/abs/2205.14135">FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</a></li>
          <li><a href="https://arxiv.org/abs/2310.01889">Ring Attention with Blockwise Transformers for Near-Infinite Context</a></li>
          <li><a href="https://arxiv.org/abs/2305.14314">QLoRA: Efficient Finetuning of Quantized LLMs</a></li>
        </ul>
      </td>
      <td>Andy Zheng<br>Ruotian Wu<br>Benjamin Schneider</td>
    </tr>
    <tr>
      <td></td>
      <td><strong>20</strong></td>
      <td><strong>Inference/Model Scalability</strong></td>
      <td>
        <ul>
          <li><a href="https://arxiv.org/abs/2309.06180">Efficient Memory Management for Large Language Model Serving with PagedAttention</a></li>
          <li><a href="https://arxiv.org/abs/2211.17192">Fast Inference from Transformers via Speculative Decoding</a></li>
          <li><a href="https://arxiv.org/abs/2305.17118">Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time</a></li>
          <li><a href="https://arxiv.org/abs/2405.12981">Reducing Transformer Key-Value Cache Size with Cross-Layer Attention</a></li>
          <li><a href="https://arxiv.org/abs/2403.05527">GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM</a></li>
          <li><a href="https://arxiv.org/abs/2208.07339">LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale</a></li>
          <li><a href="https://arxiv.org/abs/2206.01861">ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers</a></li>
          <li><a href="https://arxiv.org/abs/2009.14794">Rethinking Attention with Performers</a></li>
          <li><a href="https://arxiv.org/abs/2103.02143">Random Feature Attention</a></li>
          <li><a href="https://arxiv.org/abs/2001.04451">Reformer: The Efficient Transformer</a></li>
          <li><a href="https://arxiv.org/abs/2111.00396">Efficiently Modeling Long Sequences with Structured State Spaces</a></li>
          <li><a href="https://arxiv.org/abs/2312.00752">Mamba: Linear-Time Sequence Modeling with Selective State Spaces</a></li>
        </ul>
      </td>
      <td>Amber Wang<br>Mike Ogezi<br>Chang Liu</td>
    </tr>

    <!-- November 20 -->
    <tr>
      <td><strong>Nov 20</strong></td>
      <td><strong>21</strong></td>
      <td><strong>Reasoning</strong></td>
      <td>
        <ul>
          <li><a href="https://arxiv.org/abs/2405.14838">From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step</a></li>
          <li><a href="https://arxiv.org/abs/2305.18654">Faith and Fate: Limits of Transformers on Compositionality</a></li>
          <li><a href="https://arxiv.org/abs/2403.09629">Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking</a></li>
          <li><a href="https://arxiv.org/abs/2402.14083">Beyond A*: Better Planning with Transformers via Search Dynamics Bootstrapping</a></li>
          <li><a href="https://arxiv.org/abs/2110.14168">Training Verifiers to Solve Math Word Problems</a></li>
          <li><a href="https://arxiv.org/abs/2408.03314v1">Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters</a></li>
          <li><a href="https://arxiv.org/abs/2405.15071v2">Grokked Transformers are Implicit Reasoners: A Mechanistic Journey to the Edge of Generalization</a> (also listed under Mechanistic Interpretability)</li>
          <li><a href="https://arxiv.org/abs/2407.06023">Distilling System 2 into System 1</a></li>
          <li><a href="https://arxiv.org/abs/2402.07754">Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language Models</a></li>
          <li><a href="https://arxiv.org/abs/2404.15758">Let's Think Dot by Dot: Hidden Computation in Transformer Language Models</a></li>
          <li><a href="https://arxiv.org/abs/2406.14546">Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from Disparate Training Data</a></li>
        </ul>
      </td>
      <td>Ruoxi Ning<br>Han Zhou<br>Dinis Vitorino</td>
    </tr>
    <tr>
      <td></td>
      <td><strong>22</strong></td>
      <td><strong>Multimodality</strong></td>
      <td>
        <ul>
          <li><a href="https://arxiv.org/abs/2006.11239">Denoising Diffusion Probabilistic Models</a> (also listed under Variational Autoencoders)</li>
          <li><a href="https://arxiv.org/abs/2103.00020">Learning Transferable Visual Models From Natural Language Supervision</a></li>
          <li><a href="https://arxiv.org/abs/2204.14198">Flamingo: A Visual Language Model for Few-Shot Learning</a></li>
          <li><a href="https://arxiv.org/abs/2210.05147">Markup-to-Image Diffusion Models with Scheduled Sampling</a></li>
          <li><a href="https://arxiv.org/abs/2304.08485">Visual Instruction Tuning</a></li>
          <li><a href="https://arxiv.org/abs/2312.17172">Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action</a></li>
          <li><a href="https://arxiv.org/abs/2408.14837v1">Diffusion Models Are Real-Time Game Engines</a></li>
          <li><a href="https://www.arxiv.org/abs/2408.11039">Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model</a></li>
        </ul>
      </td>
      <td>Achint Soni<br>Songcheng Cai<br>Mike Ogezi</td>
    </tr>

    <!-- November 27 - Project Presentations -->
    <tr>
      <td><strong>Nov 27</strong></td>
      <td><strong>23</strong></td>
      <td><strong>Project Presentations</strong></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td></td>
      <td><strong>24</strong></td>
      <td><strong>Project Presentations</strong></td>
      <td></td>
      <td></td>
    </tr>

  </tbody>
</table>

<h5>Student Responsibilities:</h5>
<ul>
  <li><strong>Presentation:</strong> Collaborate with your co-presenters, choose a subset of papers, make slides, and lead your selected section.</li>
  <li><strong>Attendance:</strong> Attendance for your presentation day is mandatory. If an emergency arises, notify the instructor as soon as possible to make alternative arrangements.</li>
</ul>


<br>
<h4>Course Policies</h4>
<p><strong>Academic Integrity:</strong> All submitted work must be original. Plagiarism is not permitted.</p>
<p><strong>Attendance:</strong> Regular attendance and active participation are expected. Absences should be communicated in advance, and unexcused absences may impact the participation grade.</p>
<p><strong>Accommodations:</strong> Students requiring accommodations should reach out early in the semester to discuss necessary arrangements.</p>

<br>
<h4>Acknowledgment</h4>
<p>This syllabus was adapted from the syllabus of CS187 at Harvard, developed by my PhD co-advisor, Professor <a href="https://www.eecs.harvard.edu/shieber/">Stuart Shieber</a>. The grading structure was adapted from Professor <a href="https://pengyunie.github.io/">Pengyu Nie</a>'s course website <a href="https://pengyunie.github.io/cs846mlse-1249/">CS 846</a>.</p>
