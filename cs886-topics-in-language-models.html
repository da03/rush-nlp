---
layout: page
title: CS 886 Topics in Language Models (Fall 24)
permalink: /teaching/fall2024/cs886-topics-in-language-models/
weight: 6
---

<h3>CS 886: Topics in Language Models (Fall 24)</h3>
<p><strong>Instructor:</strong> Yuntian Deng</p>
<p><strong>Course Schedule:</strong> Wednesdays, 12:00 pm - 2:50 pm</p>
<p><strong>Location:</strong> DC 2585</p>
<p><strong>Enrollment Limit:</strong> 30 students</p>
<p><strong>Instructor Email:</strong> <a href="mailto:yuntian@uwaterloo.ca">yuntian@uwaterloo.ca</a></p>
<p><strong>Office Hours:</strong> <a href="https://calendly.com/yuntiandeng/office-hour">Schedule via Calendly</a></p>
<p style="color: red;"><strong>Note:</strong> This is a provisional version of the syllabus. Expect changes over time.</p>

<br>
<h4>Course Description</h4>
<p>This graduate seminar focuses on recent advancements in language models. In each session, students will present and discuss on recent papers in the field. The course emphasizes critical analysis, enabling students to understand the strengths, limitations, and emerging trends in language modeling.</p>

<br>
<h4>Grading</h4>
<ul>
  <li><strong>Class Participation:</strong> 20%</li>
  <li><strong>Presentations:</strong> 40%</li>
  <li><strong>Final Project:</strong> 40%</li>
</ul>

<br>
<h4>Intended Learning Outcomes</h4>
<ul>
  <li>Keep up-to-date with the current progress in language models.</li>
  <li>Critically analyze research publications.</li>
  <li>Develop and present original research ideas related to language models.</li>
</ul>

<br>
<h4>Prerequisites and Recommended Materials</h4>
<ul>
  <li>Familiarity with probabilistic theory</li>
  <li>Understanding of numeric optimization (backpropagation)</li>
  <li>Proficiency in Python, PyTorch, and tensor programming</li>
  <li>Ability to independently read and understand research papers for active participation in discussions</li>
</ul>
<p>Below is a series of exercises from my PhD advisor Professor <a href="https://rush-nlp.com/">Alexander Rush</a> that I highly recommend:</p>
<ul>
  <li><a href="https://github.com/srush/Tensor-Puzzles">Tensor Puzzles</a></li>
  <li><a href="https://github.com/srush/gpu-puzzles">GPU Puzzles</a></li>
  <li><a href="https://github.com/srush/Autodiff-Puzzles">Autodiff Puzzles</a></li>
  <li><a href="https://github.com/srush/Transformer-Puzzles">Transformer Puzzles</a></li>
  <li><a href="https://github.com/srush/GPTworld">GPT World</a></li>
  <li><a href="https://github.com/srush/LLM-Training-Puzzles">LLM Training Puzzles</a></li>
</ul>

<br>
<h4>Coursework Overview</h4>
<p><strong>Presentations:</strong> Students will present one or more papers during the course. Presentations are crucial and should not be missed, as they significantly contribute to the class dynamics. If unforeseen circumstances arise, inform the instructor as soon as possible.</p>

<p><strong>Final Project:</strong> Students will work on a final project that allows them to explore a topic related to language models in depth. Projects can be done individually or in groups of up to 3 members. Groups larger than this range require a justification and are subject to instructor approval.</p>

<ul>
  <li><strong>Original Contribution:</strong> The primary aim of the project is to make an original contribution to the field. Ideally, the project should be of a quality that could potentially lead to a publication. This could involve, but is not limited to:
    <ul>
      <li><strong>Novel Research:</strong> Developing new methods, models, or insights in the field of language models.</li>
      <li><strong>Reproducibility Test:</strong> Attempting to replicate and possibly extend the results from a published paper, providing detailed analysis and commentary on the findings.</li>
      <li><strong>Negative Results:</strong> Investigating a hypothesis or method that did not yield expected results, with thorough documentation of the process and analysis of why it failed.</li>
      <li><strong>Survey:</strong> Conducting a comprehensive and critical survey of a specific area within language models, identifying gaps or trends that could inform future research. Even for surveys, the work should go beyond summarization to include thoughtful analysis and synthesis of the literature.</li>
    </ul>
  </li>
  <li><strong>Project Timeline:</strong> 
    <ul>
      <li><strong>Proposal Submission:</strong> Mid-semester, students will submit a project proposal outlining their research question, hypothesis, and approach.</li>
      <li><strong>Final Presentation:</strong> At the end of the semester, each group will present their findings to the class.</li>
      <li><strong>Final Report:</strong> A detailed report summarizing the project, including research question, hypothesis, approach, results, and discussion, will be due after the presentation.</li>
    </ul>
  </li>
</ul>

<p>Participation in every class is expected since discussion is crucial to the seminar format.</p>

<h4>Syllabus and Presentation Sign-Up</h4>
<p>Each class session is divided into two sections. Each student must sign up for <strong>two presentations by Sep 9</strong>. Sign up in pairs (two students per section), but if (and only if) all available pairs are full, you may sign up as the third member of a group.</p>

<h5>Sign-up Instructions:</h5>
<ul>
  <li>Sign up for <strong>two presentation slots</strong> to receive full credit.</li>
  <li>Each section allows up to two students to sign up initially (one for each column).</li>
  <li>Only if no groups have fewer than two students, sign up as the third student in the <strong>Optional Student 3</strong> column.</li>
  <li>Use the embedded Google Sheet below to sign up for presentations by commenting in cells you want to sign up with your name (only first comment in each cell will be valid).</li>
</ul>

<h5>Google Sheet Sign-up:</h5>
<center><iframe width="600" height="700" src="https://docs.google.com/spreadsheets/d/1aCSNww2wIqajGpZOvhSvcudg5wBhIqdzR4ZiJzz282s/edit?usp=sharing?widget=true&amp;single=true&amp;"></iframe></center>


<!-- Syllabus Table -->
<table class="table table-striped">
  <thead>
    <tr>
      <th>Date</th>
      <th>Section #</th>
      <th>Section Topic</th>
      <th>Recommended Papers</th>
      <th>Student 1</th>
      <th>Student 2</th>
      <th>Optional Student 3</th>
    </tr>
  </thead>
  <tbody>

    <!-- September 4 -->
    <tr>
      <td><strong>Sep 4</strong></td>
      <td><strong>1</strong></td>
      <td><strong>Introduction Lecture</strong></td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td></td>
      <td><strong>2</strong></td>
      <td><strong>Course Logistics</strong></td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>

    <!-- September 11 -->
    <tr>
      <td><strong>Sep 11</strong></td>
      <td><strong>3</strong></td>
      <td><strong>Sequence-to-Sequence Learning</strong></td>
      <td>
        <ul>
          <li><a href="https://arxiv.org/abs/1301.3781">Efficient Estimation of Word Representations in Vector Space</a></li>
          <li><a href="https://arxiv.org/abs/1409.0473">Neural Machine Translation by Jointly Learning to Align and Translate</a></li>
          <li><a href="https://arxiv.org/abs/1502.03044">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention</a></li>
          <li><a href="https://aclanthology.org/D14-1162/">GloVe: Global Vectors for Word Representation</a></li>
          <li><a href="https://arxiv.org/abs/1409.3215">Sequence to Sequence Learning with Neural Networks</a></li>
          <li><a href="https://arxiv.org/abs/1103.0398">Natural Language Processing (almost) from Scratch</a></li>
          <li><a href="https://arxiv.org/abs/1508.04025">Effective Approaches to Attention-based Neural Machine Translation</a></li>
          <li><a href="https://arxiv.org/abs/1508.01211">Listen, Attend and Spell</a></li>
        </ul>
      </td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td></td>
      <td><strong>4</strong></td>
      <td><strong>Optimization</strong></td>
      <td>
        <ul>
          <li><a href="https://arxiv.org/abs/1412.6980">Adam: A Method for Stochastic Optimization</a></li>
          <li><a href="https://arxiv.org/abs/1711.05101">Decoupled Weight Decay Regularization</a></li>
          <li><a href="https://arxiv.org/abs/1511.06732">Sequence Level Training with Recurrent Neural Networks</a></li>
          <li><a href="https://arxiv.org/abs/1610.09038">Professor Forcing: A New Algorithm for Training Recurrent Networks</a></li>
          <li><a href="https://arxiv.org/abs/1803.03635">The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks</a></li>
          <li><a href="https://arxiv.org/abs/1606.02960">Sequence-to-Sequence Learning as Beam-Search Optimization</a></li>
          <li><a href="https://arxiv.org/abs/1606.04474">Learning to learn by gradient descent by gradient descent</a></li>
          <li><a href="https://arxiv.org/abs/1611.03530">Understanding deep learning requires rethinking generalization</a></li>
        </ul>
      </td>
      <td></td>
      <td></td>
      <td></td>
    </tr>

    <!-- September 18 -->
    <tr>
      <td><strong>Sep 18</strong></td>
      <td><strong>5</strong></td>
      <td><strong>Variational Autoencoders</strong></td>
      <td>
        <ul>
          <li><a href="https://arxiv.org/abs/1312.6114">Auto-Encoding Variational Bayes</a></li>
          <li><a href="https://arxiv.org/abs/1511.06349">Generating Sentences from a Continuous Space</a></li>
          <li><a href="https://arxiv.org/abs/1703.00955">Toward Controlled Generation of Text</a></li>
          <li><a href="https://arxiv.org/abs/1807.03756">Latent Alignment and Variational Attention</a></li>
          <li><a href="https://arxiv.org/abs/1904.03746">Unsupervised Recurrent Neural Network Grammars</a></li>
          <li><a href="https://arxiv.org/abs/1901.05534">Lagging Inference Networks and Posterior Collapse in Variational Autoencoders</a></li>
          <li><a href="https://arxiv.org/abs/2006.11239">Denoising Diffusion Probabilistic Models</a></li>
        </ul>
      </td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td></td>
      <td><strong>6</strong></td>
      <td><strong>Knowledge Distillation</strong></td>
      <td>
        <ul>
          <li><a href="https://arxiv.org/abs/1503.02531">Distilling the Knowledge in a Neural Network</a></li>
          <li><a href="https://arxiv.org/abs/1606.07947">Sequence-Level Knowledge Distillation</a></li>
          <li><a href="https://arxiv.org/abs/1910.12366">Thieves on Sesame Street! Model Extraction of BERT-based APIs</a></li>
          <li><a href="https://arxiv.org/abs/2209.15189">Learning by Distilling Context</a></li>
          <li><a href="https://arxiv.org/abs/2311.01460">Implicit Chain of Thought Reasoning via Knowledge Distillation</a></li>
          <li><a href="https://arxiv.org/abs/2110.07178">Symbolic Knowledge Distillation: from General Language Models to Commonsense Models</a></li>
          <li><a href="https://arxiv.org/abs/2305.16635">Impossible Distillation: from Low-Quality Model to High-Quality Dataset &amp; Model for Summarization and Paraphrasing</a></li>
          <li><a href="https://arxiv.org/abs/2403.13780">Information-Theoretic Distillation for Reference-less Summarization</a></li>
        </ul>
      </td>
      <td></td>
      <td></td>
      <td></td>
    </tr>

    <!-- September 25 -->
    <tr>
      <td><strong>Sep 25</strong></td>
      <td><strong>7</strong></td>
      <td><strong>Transformer</strong></td>
      <td>
        <ul>
          <li><a href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a></li>
          <li><a href="https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a></li>
          <li><a href="https://arxiv.org/abs/1607.06450">Layer Normalization</a></li>
          <li><a href="https://arxiv.org/abs/1512.03385">Deep Residual Learning for Image Recognition</a></li>
          <li><a href="https://arxiv.org/abs/1608.05859">Using the Output Embedding to Improve Language Models</a></li>
          <li><a href="https://arxiv.org/abs/2403.06634">Stealing Part of a Production Language Model</a></li>
          <li><a href="https://arxiv.org/abs/2403.09539">Logits of API-Protected LLMs Leak Proprietary Information</a></li>
        </ul>
      </td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td></td>
      <td><strong>8</strong></td>
      <td><strong>Pretraining</strong></td>
      <td>
        <ul>
          <li><a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></li>
          <li><a href="https://arxiv.org/abs/1802.05365">Deep contextualized word representations</a></li>
          <li><a href="https://arxiv.org/abs/1910.10683">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a></li>
          <li><a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Language Models are Unsupervised Multitask Learners</a></li>
          <li><a href="https://arxiv.org/abs/2204.02311">PaLM: Scaling Language Modeling with Pathways</a></li>
          <li><a href="https://arxiv.org/abs/2407.21783">The Llama 3 Herd of Models</a></li>
          <li><a href="https://arxiv.org/abs/2205.01068">OPT: Open Pre-trained Transformer Language Models</a></li>
          <li><a href="https://arxiv.org/abs/2211.05100">BLOOM: A 176B-Parameter Open-Access Multilingual Language Model</a></li>
          <li><a href="https://arxiv.org/abs/1910.13461">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</a></li>
          <li><a href="https://arxiv.org/abs/1906.08237">XLNet: Generalized Autoregressive Pretraining for Language Understanding</a></li>
          <li><a href="https://arxiv.org/abs/2003.10555">ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators</a></li>
        </ul>
      </td>
      <td></td>
      <td></td>
      <td></td>
    </tr>

    <!-- October 2 -->
    <tr>
      <td><strong>Oct 2</strong></td>
      <td><strong>9</strong></td>
      <td><strong>Non-Autoregressive Models</strong></td>
      <td>
        <ul>
          <li><a href="https://arxiv.org/abs/1711.02281">Non-Autoregressive Neural Machine Translation</a></li>
          <li><a href="https://arxiv.org/abs/1802.06901">Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement</a></li>
          <li><a href="https://arxiv.org/abs/1904.09324">Mask-Predict: Parallel Decoding of Conditional Masked Language Models</a></li>
          <li><a href="https://arxiv.org/abs/2006.10369">Deep Encoder, Shallow Decoder: Reevaluating Non-autoregressive Machine Translation</a></li>
          <li><a href="https://arxiv.org/abs/1911.02727">Understanding Knowledge Distillation in Non-autoregressive Machine Translation</a></li>
          <li><a href="https://arxiv.org/abs/1909.02480">FlowSeq: Non-Autoregressive Conditional Sequence Generation with Generative Flow</a></li>
          <li><a href="https://arxiv.org/abs/2006.01112">Cascaded Text Generation with Markov Transformers</a></li>
          <li><a href="https://arxiv.org/abs/2004.11714">Residual Energy-Based Models for Text Generation</a></li>
        </ul>
      </td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td></td>
      <td><strong>10</strong></td>
      <td><strong>Evaluation 1</strong></td>
      <td>
        <ul>
          <li><a href="https://arxiv.org/abs/1804.07461">GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding</a></li>
          <li><a href="https://arxiv.org/abs/1502.05698">Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks</a></li>
          <li><a href="https://arxiv.org/abs/1804.06028">ListOps: A Diagnostic Dataset for Latent Tree Learning</a></li>
          <li><a href="https://arxiv.org/abs/1711.00350">Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks</a></li>
          <li><a href="https://arxiv.org/abs/1910.14599">Adversarial NLI: A New Benchmark for Natural Language Understanding</a></li>
          <li><a href="https://arxiv.org/abs/2210.08444">Model Criticism for Long-Form Text Generation</a></li>
          <li><a href="https://arxiv.org/abs/2102.01454">MAUVE: Measuring the Gap Between Neural Text and Human Text using Divergence Frontiers</a></li>
        </ul>
      </td>
      <td></td>
      <td></td>
      <td></td>
    </tr>

    <!-- October 9 -->
    <tr>
      <td><strong>Oct 9</strong></td>
      <td><strong>11</strong></td>
      <td><strong>Interpretability</strong></td>
      <td>
        <ul>
          <li><a href="https://arxiv.org/abs/1506.02078">Visualizing and Understanding Recurrent Networks</a></li>
          <li><a href="https://arxiv.org/abs/1503.02406">Deep Learning and the Information Bottleneck Principle</a></li>
          <li><a href="https://arxiv.org/abs/1704.03471">What do Neural Machine Translation Models Learn about Morphology?</a></li>
          <li><a href="https://arxiv.org/abs/1909.03368">Designing and Interpreting Probes with Control Tasks</a></li>
          <li><a href="https://arxiv.org/pdf/2106.00737">Implicit Representations of Meaning in Neural Language Models</a></li>
          <li><a href="https://aclanthology.org/N19-1357/">Attention is not Explanation</a></li>
          <li><a href="https://arxiv.org/abs/1908.04626">Attention is not not Explanation</a></li>
          <li><a href="https://arxiv.org/abs/2109.06387">Rationales for Sequential Predictions</a></li>
          <li><a href="https://arxiv.org/abs/2306.12672">From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought</a></li>
          <li><a href="https://arxiv.org/abs/2210.13382">Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task</a></li>
          <li><a href="https://transformer-circuits.pub/2024/scaling-monosemanticity/">Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet</a></li>
          <li><a href="https://arxiv.org/abs/2406.04093">Scaling and evaluating sparse autoencoders</a></li>
        </ul>
      </td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td></td>
      <td><strong>12</strong></td>
      <td><strong>Mechanistic Interpretability</strong></td>
      <td>
        <ul>
          <li><a href="https://physics.allen-zhu.com/">Physics of Language Models</a></li>
          <li><a href="https://arxiv.org/abs/2406.03445v1">Pre-trained Large Language Models Use Fourier Features to Compute Addition</a></li>
          <li><a href="https://arxiv.org/abs/2407.11421v1">States Hidden in Hidden States: LLMs Emerge Discrete State Representations Implicitly</a></li>
          <li><a href="https://arxiv.org/abs/2405.15071v2">Grokked Transformers are Implicit Reasoners: A Mechanistic Journey to the Edge of Generalization</a></li>
          <li><a href="https://arxiv.org/abs/2211.07349">Finding Skill Neurons in Pre-trained Transformer-based Language Models</a></li>
          <li><a href="https://arxiv.org/abs/2201.02177">Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets</a></li>
        </ul>
      </td>
      <td></td>
      <td></td>
      <td></td>
    </tr>

    <!-- Reading Week -->
    <!--tr>
      <td><strong>Oct 12-20</strong></td>
      <td colspan="7"><strong>Reading Week - No Class</strong></td>
    </tr-->

    <!-- October 23 -->
    <tr>
      <td><strong>Oct 23</strong></td>
      <td><strong>13</strong></td>
      <td><strong>Knowledge Editing</strong></td>
      <td>
        <ul>
          <li><a href="https://arxiv.org/abs/2202.05262">Locating and Editing Factual Associations in GPT</a></li>
          <li><a href="https://arxiv.org/abs/2306.03341">Inference-Time Intervention: Eliciting Truthful Answers from a Language Model</a></li>
          <li><a href="https://arxiv.org/abs/2104.08696">Knowledge neurons in pretrained transformers</a></li>
          <li><a href="https://arxiv.org/abs/2304.00740">Inspecting and Editing Knowledge Representations in Language Models</a></li>
        </ul>
      </td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td></td>
      <td><strong>14</strong></td>
      <td><strong>Adversarial Robustness</strong></td>
      <td>
        <ul>
          <li><a href="https://arxiv.org/abs/1312.6199">Intriguing properties of neural networks</a></li>
          <li><a href="https://arxiv.org/abs/1412.6572">Explaining and Harnessing Adversarial Examples</a></li>
          <li><a href="https://arxiv.org/abs/2307.15043">Universal and Transferable Adversarial Attacks on Aligned Language Models</a></li>
          <li><a href="https://arxiv.org/abs/2404.13208v1">The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions</a></li>
          <li><a href="https://arxiv.org/abs/1905.02175">Adversarial Examples Are Not Bugs, They Are Features</a></li>
          <li><a href="https://arxiv.org/abs/1908.07125">Universal Adversarial Triggers for Attacking and Analyzing NLP</a></li>
          <li><a href="https://arxiv.org/abs/1707.07328">Adversarial Examples for Evaluating Reading Comprehension Systems</a></li>
          <li><a href="https://arxiv.org/abs/2005.05909">Textattack: A framework for adversarial attacks, data augmentation, and adversarial training in nlp</a></li>
        </ul>
      </td>
      <td></td>
      <td></td>
      <td></td>
    </tr>

    <!-- October 30 -->
    <tr>
      <td><strong>Oct 30</strong></td>
      <td><strong>15</strong></td>
      <td><strong>Prompting</strong></td>
      <td>
        <ul>
          <li><a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a></li>
          <li><a href="https://arxiv.org/abs/2201.11903">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a></li>
          <li><a href="https://arxiv.org/abs/2210.03629">ReAct: Synergizing Reasoning and Acting in Language Models</a></li>
          <li><a href="https://arxiv.org/abs/2305.10601">Tree of Thoughts: Deliberate Problem Solving with Large Language Models</a></li>
          <li><a href="https://arxiv.org/abs/2310.14034">Tree Prompting: Efficient Task Adaptation without Fine-Tuning</a></li>
          <li><a href="https://arxiv.org/abs/2211.11559">Visual Programming: Compositional visual reasoning without training</a></li>
          <li><a href="https://arxiv.org/abs/2112.00114">Show Your Work: Scratchpads for Intermediate Computation with Language Models</a></li>
          <li><a href="https://arxiv.org/abs/2310.11324">Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting</a></li>
          <li><a href="https://arxiv.org/abs/2211.12588">Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks</a></li>
          <li><a href="https://arxiv.org/abs/2406.09403">Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models</a></li>
          <li><a href="https://arxiv.org/abs/2005.11401">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</a></li>
          <li><a href="https://arxiv.org/abs/2309.03882">Large Language Models Are Not Robust Multiple Choice Selectors</a></li>
          <li><a href="https://aclanthology.org/2023.emnlp-demo.27/">MiniChain: A Small Library for Coding with Large Language Models</a></li>
        </ul>
      </td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td></td>
      <td><strong>16</strong></td>
      <td><strong>Prompt Engineering</strong></td>
      <td>
        <ul>
          <li><a href="https://arxiv.org/abs/2010.15980">AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts</a></li>
          <li><a href="https://arxiv.org/abs/2406.07496">TextGrad: Automatic "Differentiation" via Text</a></li>
          <li><a href="https://arxiv.org/abs/2310.03714">DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines</a></li>
          <li><a href="https://openreview.net/forum?id=GvMuB-YsiK6">Explaining Patterns in Data with Language Models via Interpretable Autoprompting</a></li>
          <li><a href="https://arxiv.org/abs/2201.12323">Describing Differences between Text Distributions with Natural Language</a></li>
        </ul>
      </td>
      <td></td>
      <td></td>
      <td></td>
    </tr>

    <!-- November 6 -->
    <tr>
      <td><strong>Nov 6</strong></td>
      <td><strong>17</strong></td>
      <td><strong>Alignment</strong></td>
      <td>
        <ul>
          <li><a href="https://arxiv.org/abs/2203.02155">Training language models to follow instructions with human feedback</a></li>
          <li><a href="https://arxiv.org/abs/2312.01552">The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning</a></li>
          <li><a href="https://arxiv.org/abs/2009.01325">Learning to summarize from human feedback</a></li>
          <li><a href="https://arxiv.org/abs/2305.18290">Direct Preference Optimization: Your Language Model is Secretly a Reward Model</a></li>
          <li><a href="https://arxiv.org/abs/2212.10560">Self-Instruct: Aligning Language Models with Self-Generated Instructions</a></li>
          <li><a href="https://arxiv.org/abs/2406.08464">Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing</a></li>
          <li><a href="https://arxiv.org/abs/2309.05653">MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning</a></li>
        </ul>
      </td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td></td>
      <td><strong>18</strong></td>
      <td><strong>Evaluation 2</strong></td>
      <td>
        <ul>
          <li><a href="https://arxiv.org/abs/2306.05685">Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</a></li>
          <li><a href="https://arxiv.org/abs/2107.03374">Evaluating Large Language Models Trained on Code</a></li>
          <li><a href="https://arxiv.org/abs/2310.06770">SWE-bench: Can Language Models Resolve Real-World GitHub Issues?</a></li>
          <li><a href="https://arxiv.org/abs/2009.03300">Measuring Massive Multitask Language Understanding</a></li>
          <li><a href="https://arxiv.org/abs/2303.11897">TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation with Question Answering</a></li>
          <li><a href="https://arxiv.org/abs/2403.13787">RewardBench: Evaluating Reward Models for Language Modeling</a></li>
          <li><a href="https://arxiv.org/abs/2311.12022">GPQA: A Graduate-Level Google-Proof Q&A Benchmark</a></li>
          <li><a href="https://arxiv.org/abs/2406.01574">MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark</a></li>
          <li><a href="https://arxiv.org/abs/2305.14251">FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation</a></li>
          <li><a href="https://arxiv.org/abs/2407.18370">Trust or Escalate: LLM Judges with Provable Guarantees for Human Agreement</a></li>
          <li><a href="https://arxiv.org/abs/2406.04770">WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild</a></li>
          <li><a href="https://arxiv.org/abs/2311.16502">MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI</a></li>
          <li><a href="https://arxiv.org/abs/2404.12390">BLINK: Multimodal Large Language Models Can See but Not Perceive</a></li>
        </ul>
      </td>
      <td></td>
      <td></td>
      <td></td>
    </tr>

    <!-- November 13 -->
    <tr>
      <td><strong>Nov 13</strong></td>
      <td><strong>19</strong></td>
      <td><strong>Training Scalability</strong></td>
      <td>
        <ul>
          <li><a href="https://arxiv.org/abs/2106.09685">LoRA: Low-Rank Adaptation of Large Language Models</a></li>
          <li><a href="https://dl.acm.org/doi/10.1145/3394486.3406703">DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters</a></li>
          <li><a href="https://arxiv.org/abs/2204.06745">GPT-NeoX-20B: An Open-Source Autoregressive Language Model</a></li>
          <li><a href="https://arxiv.org/abs/1909.08053v4">Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</a></li>
          <li><a href="https://arxiv.org/abs/2203.15556">Training Compute-Optimal Large Language Models</a></li>
          <li><a href="https://arxiv.org/abs/2205.14135">FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</a></li>
          <li><a href="https://arxiv.org/abs/2310.01889">Ring Attention with Blockwise Transformers for Near-Infinite Context</a></li>
          <li><a href="https://arxiv.org/abs/2305.14314">QLoRA: Efficient Finetuning of Quantized LLMs</a></li>
        </ul>
      </td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td></td>
      <td><strong>20</strong></td>
      <td><strong>Inference and Model Scalability</strong></td>
      <td>
        <ul>
          <li><a href="https://arxiv.org/abs/2309.06180">Efficient Memory Management for Large Language Model Serving with PagedAttention</a></li>
          <li><a href="https://arxiv.org/abs/2211.17192">Fast Inference from Transformers via Speculative Decoding</a></li>
          <li><a href="https://arxiv.org/abs/2305.17118">Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time</a></li>
          <li><a href="https://arxiv.org/abs/2405.12981">Reducing Transformer Key-Value Cache Size with Cross-Layer Attention</a></li>
          <li><a href="https://arxiv.org/abs/2403.05527">GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM</a></li>
          <li><a href="https://arxiv.org/abs/2208.07339">LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale</a></li>
          <li><a href="https://arxiv.org/abs/2206.01861">ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers</a></li>
          <li><a href="https://arxiv.org/abs/2009.14794">Rethinking Attention with Performers</a></li>
          <li><a href="https://arxiv.org/abs/2103.02143">Random Feature Attention</a></li>
          <li><a href="https://arxiv.org/abs/2001.04451">Reformer: The Efficient Transformer</a></li>
          <li><a href="https://arxiv.org/abs/2111.00396">Efficiently Modeling Long Sequences with Structured State Spaces</a></li>
          <li><a href="https://arxiv.org/abs/2312.00752">Mamba: Linear-Time Sequence Modeling with Selective State Spaces</a></li>
        </ul>
      </td>
      <td></td>
      <td></td>
      <td></td>
    </tr>

    <!-- November 20 -->
    <tr>
      <td><strong>Nov 20</strong></td>
      <td><strong>21</strong></td>
      <td><strong>Reasoning</strong></td>
      <td>
        <ul>
          <li><a href="https://arxiv.org/abs/2405.14838">From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step</a></li>
          <li><a href="https://arxiv.org/abs/2305.18654">Faith and Fate: Limits of Transformers on Compositionality</a></li>
          <li><a href="https://arxiv.org/abs/2403.09629">Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking</a></li>
          <li><a href="https://arxiv.org/abs/2402.14083">Beyond A*: Better Planning with Transformers via Search Dynamics Bootstrapping</a></li>
          <li><a href="https://arxiv.org/abs/2110.14168">Training Verifiers to Solve Math Word Problems</a></li>
          <li><a href="https://arxiv.org/abs/2408.03314v1">Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters</a></li>
          <li><a href="https://arxiv.org/abs/2405.15071v2">Grokked Transformers are Implicit Reasoners: A Mechanistic Journey to the Edge of Generalization</a> (also listed under Mechanistic Interpretability)</li>
          <li><a href="https://arxiv.org/abs/2407.06023">Distilling System 2 into System 1</a></li>
          <li><a href="https://arxiv.org/abs/2402.07754">Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language Models</a></li>
          <li><a href="https://arxiv.org/abs/2404.15758">Let's Think Dot by Dot: Hidden Computation in Transformer Language Models</a></li>
          <li><a href="https://arxiv.org/abs/2406.14546">Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from Disparate Training Data</a></li>
        </ul>
      </td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td></td>
      <td><strong>22</strong></td>
      <td><strong>Multimodality</strong></td>
      <td>
        <ul>
          <li><a href="https://arxiv.org/abs/2006.11239">Denoising Diffusion Probabilistic Models</a> (also listed under Variational Autoencoders)</li>
          <li><a href="https://arxiv.org/abs/2204.14198">Flamingo: A Visual Language Model for Few-Shot Learning</a></li>
          <li><a href="https://arxiv.org/abs/2210.05147">Markup-to-Image Diffusion Models with Scheduled Sampling</a></li>
          <li><a href="https://arxiv.org/abs/2304.08485">Visual Instruction Tuning</a></li>
          <li><a href="https://arxiv.org/abs/2312.17172">Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action</a></li>
          <li><a href="https://arxiv.org/abs/2408.14837v1">Diffusion Models Are Real-Time Game Engines</a></li>
          <li><a href="https://www.arxiv.org/abs/2408.11039">Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model</a></li>
        </ul>
      </td>
      <td></td>
      <td></td>
      <td></td>
    </tr>

    <!-- November 27 - Project Presentations -->
    <tr>
      <td><strong>Nov 27</strong></td>
      <td><strong>23</strong></td>
      <td><strong>Project Presentations 1</strong></td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td></td>
      <td><strong>24</strong></td>
      <td><strong>Project Presentations 2</strong></td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>

  </tbody>
</table>

<h5>Student Responsibilities:</h5>
<ul>
  <li><strong>Presentation:</strong> Collaborate with your co-presenters, choose a subset of papers, make slides, and lead your selected section.</li>
  <li><strong>Attendance:</strong> Attendance for your presentation day is mandatory. If an emergency arises, notify the instructor as soon as possible to make alternative arrangements.</li>
</ul>


<br>
<h4>Course Policies</h4>
<p><strong>Academic Integrity:</strong> All submitted work must be original. Plagiarism is not permitted.</p>
<p><strong>Attendance:</strong> Regular attendance and active participation are expected. Absences should be communicated in advance, and unexcused absences may impact the participation grade.</p>
<p><strong>Accommodations:</strong> Students requiring accommodations should reach out early in the semester to discuss necessary arrangements.</p>

<br>
<h4>Acknowledgment</h4>
<p>This syllabus was adapted from the syllabus of CS187 at Harvard, developed by my PhD co-advisor, Professor <a href="https://www.eecs.harvard.edu/shieber/">Stuart Shieber</a>.</p>
