---
layout: page
title: Main
weight: 1.5
show: 1
---

<script>
    function toggleNews() {
        const newsList = document.getElementById('news-list');
        const newsItems = newsList.querySelectorAll('li');
        const toggleBtn = document.getElementById('toggle-news-btn');
        const hiddenClass = 'hidden-news';

        if (toggleBtn.textContent === 'Show Older News') {
            newsItems.forEach(item => item.classList.remove(hiddenClass));
            toggleBtn.textContent = 'Hide Older News';
        } else {
            newsItems.forEach((item, index) => {
                if (index >= 5) {
                    item.classList.add(hiddenClass);
                }
            });
            toggleBtn.textContent = 'Show Older News';
        }
    }

    document.addEventListener('DOMContentLoaded', () => {
        toggleNews();
    });
</script>

<div class="col-7 text-center">
  <table>
<tr><td style="padding:10px 90px 10px 0px">
      <img width="200" class="img-rounded" src="https://avatars.githubusercontent.com/u/5753959?v=4" alt="Yuntian Deng">
  </td><td style="padding:10px">
    <h1>Yuntian Deng</h1>
    <b>Assistant Professor</b>, UWaterloo<br>
    <b>Visiting Professor</b>, NVIDIA<br>
    <b>Associate</b>, Harvard SEAS<br>
    <b>Faculty Affiliate</b>, Vector Institute<br>
    PhD in CS, Harvard<br>
    <span style="font-size:16px"><a href="cv/cv.comp.pdf">[CV]</a> <a href="https://scholar.google.com/citations?user=tk0e5lYAAAAJ&hl=en&oi=ao">[Google Scholar]</a> <a href="https://twitter.com/yuntiandeng">[Twitter]</a></span> <br>

      </td>
  </tr>
  </table>
<br>
</div>


<p style="font-size: 16px">
I am an assistant professor at the University of Waterloo and a visiting professor at NVIDIA under the supervision of Prof. <a href="https://homes.cs.washington.edu/~yejin/">Yejin Choi</a>. My research interests are Natural Language Processing and Machine Learning. I also enjoy building <a href="demos/">demos</a> such as <a href="https://neural-os.com">NeuralOS</a>, <a href="https://wildvisualizer.com">WildVis</a>, <a href="https://huggingface.co/spaces/yuntian-deng/ChatGPT4">WildChat</a>, <a href="https://huggingface.co/spaces/yuntian-deng/gpt2-multiplication">Multiplication Predictor w/o CoT</a>, <a href="https://huggingface.co/spaces/yuntian-deng/implicit-cot-math">Grade School Math Solver w/o CoT</a>, <a href="https://openaiwatch.com/">OpenAI Watch</a>, <a href="https://steganography.live">Linguistic Steganography</a>, <a href="https://huggingface.co/spaces/yuntian-deng/AKSelectionPredictor">AKSelectionPredictor</a>, <a href="http://opennmt.net">OpenNMT</a>, <a href="https://huggingface.co/spaces/yuntian-deng/latex2im">Markup-to-Image Diffusion</a>, and <a href="https://im2markup.yuntiandeng.com/">Image-to-Markup</a>. I received my PhD from Harvard University, where I was advised by Prof. <a href="https://rush-nlp.com/">Alexander Rush</a> and Prof. <a href="https://www.eecs.harvard.edu/shieber/">Stuart Shieber</a>. I did a postdoc under the supervision of Prof. <a href="https://homes.cs.washington.edu/~yejin/">Yejin Choi</a>.
</p>

<br>
<h4>News</h4>
<ul style="font-size: 16px" id="news-list">
    <li>Aug 11, 2025: Released <a href="https://huggingface.co/datasets/allenai/WildChat-4.8M">WildChat-4.8M</a>, a dataset of 4.8M real user-ChatGPT conversations collected from our public chatbots, including 122K from reasoning models (o1-preview, o1-mini) and 2.5M from GPT-4o. These conversations represent real uses in the wild and are very costly to collect.
        <a class="twitter-link" href="https://x.com/yuntiandeng/status/1954929005305414062">
            <img src="images/twitter_logo.svg" alt="Tweet">
        </a>
    </li>
    <li>Jul 14, 2025: We built a demo, <a href="https://neural-os.com">NeuralOS</a>, an operating system entirely powered by neural networks. Check out our <a href="https://huggingface.co/papers/2507.08800">paper</a> for more details!
        <a class="twitter-link" href="https://x.com/yuntiandeng/status/1944802154314916331">
            <img src="images/twitter_logo.svg" alt="Tweet">
        </a>
    </li>
    <li>Nov 30, 2024: Featured in an <a href="https://www.nzz.ch/technologie/schreibe-eine-erotische-kurzgeschichte-warum-sollte-man-in-wollishofen-wohnen-wieso-versteht-mich-meine-frau-nicht-was-leute-chat-gpt-wirklich-fragen-ld.1858229">NZZ article</a> about <a href="https://wildvisualizer.com/">WildChat</a>.
        <a class="twitter-link" href="https://x.com/NZZTech/status/1862724482768490827">
            <img src="images/twitter_logo.svg" alt="Tweet">
        </a>
    </li>
    <li>Nov 20, 2024: Gave a talk at <a href="https://cims.nyu.edu/ai/seminars/cilvr-seminar-series/56/">NYU CILVR</a> on <a href="https://huggingface.co/spaces/yuntian-deng/gpt2-multiplication">implicit chain of thought reasoning</a>.
        <a class="twitter-link" href="https://twitter.com/NYUDataScience/status/1864402586427670752">
            <img src="images/twitter_logo.svg" alt="Tweet">
        </a>
    </li>
    <li>Oct 2, 2024: Interviewed by <a href="https://techcrunch.com/2024/10/02/why-is-chatgpt-so-bad-at-math">TechCrunch</a> to discuss the challenges of using LLMs to solve basic math problems.
        <a class="twitter-link" href="https://x.com/yuntiandeng/status/1836114401213989366">
            <img src="images/twitter_logo.svg" alt="Tweet">
        </a>
    </li>
    <li>Aug 4, 2024: <a href="https://huggingface.co/datasets/allenai/WildChat-1M">WildChat</a> was featured in the <a href="https://wapo.st/3A6e4me">Washington Post</a>! This work collected 1M real-world user-ChatGPT conversations. You can explore all data at <a href="https://wildvisualizer.com">wildvisualizer.com</a>, or try our data-collecting chatbot at <a href="https://huggingface.co/spaces/yuntian-deng/ChatGPT4">huggingface.co/spaces/yuntian-deng/ChatGPT4</a>.
        <a class="twitter-link" href="https://x.com/washingtonpost/status/1820111621101936692">
            <img src="images/twitter_logo.svg" alt="Tweet">
        </a>
    </li>
    <li>Jul 19, 2024: I built a <a href="https://huggingface.co/spaces/yuntian-deng/gpt2-multiplication">demo</a> using GPT-2 to directly produce the product of two numbers without chain-of-thought (CoT). CoT is internalized using the approach in <a href="https://arxiv.org/pdf/2405.14838">From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step</a>. A 12-layer GPT-2 can solve <strong>20-digit multiplication</strong> with 99.5% accuracy!
        <a class="twitter-link" href="https://twitter.com/yuntiandeng/status/1814319104448467137">
            <img src="images/twitter_logo.svg" alt="Tweet">
        </a>
    </li>
    <li>Jul 11, 2024: I built a <a href="https://huggingface.co/spaces/yuntian-deng/implicit-cot-math">demo</a> to solve grade school math problems (GSM8K)  without chain-of-thought (CoT) at 52% accuracy. CoT is internalized using the approach in <a href="https://arxiv.org/pdf/2405.14838">From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step</a>.
        <a class="twitter-link" href="https://twitter.com/yuntiandeng/status/1811448907161043224">
            <img src="images/twitter_logo.svg" alt="Tweet">
        </a>
    </li>
    <li>Jun 19, 2024: I built a website, <a href="https://wildvisualizer.com">wildvisualizer.com</a>, for interactive search of WildChat, allowing keyword, toxicity, IP, language, and country-based searches of 1M WildChat conversations.
        <a class="twitter-link" href="https://twitter.com/yuntiandeng/status/1803496908259991963">
            <img src="images/twitter_logo.svg" alt="Tweet">
        </a>
    </li>
    <li>May 29, 2024: Our paper, <a href="https://arxiv.org/pdf/2405.14838">From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step</a>, is now publicly available! This paper proposes a simple yet effective method to teach language models to internalize chain-of-thought reasoning by gradually removing intermediate steps and finetuning.
        <a class="twitter-link" href="https://twitter.com/yuntiandeng/status/1795854740879774036">
            <img src="images/twitter_logo.svg" alt="Tweet">
        </a>
    </li>
    <li>Apr 26, 2024: I built a demo, <a href="https://huggingface.co/spaces/yuntian-deng/AKSelectionPredictor">AKSelectionPredictor</a>, to predict whether a paper will be selected by <a href="https://twitter.com/_akhaliq">@_akhaliq</a> into Hugging Face papers based on its title, abstract, and authors.
        <a class="twitter-link" href="https://twitter.com/yuntiandeng/status/1783888775950561673">
            <img src="images/twitter_logo.svg" alt="Tweet">
        </a>
    </li>
    <li>Mar 5, 2024: Our dataset, <a href="https://wildchat.allen.ai">WildChat</a>, is used in Anthropic's <a href="https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf">Claude 3</a> for evaluating refusals.
        <a class="twitter-link" href="https://twitter.com/yuntiandeng/status/1765127045246284084">
            <img src="images/twitter_logo.svg" alt="Tweet">
        </a>
    </li>
    <li>Nov 14, 2023: Our dataset, <a href="https://wildchat.allen.ai">WildChat</a>, is now publicly available! It is a corpus of 650K real-world user-ChatGPT interactions, characterized by over 60 languages and a diversity of user prompts.
        <a class="twitter-link" href="https://twitter.com/yuntiandeng/status/1724503257601458575">
            <img src="images/twitter_logo.svg" alt="Tweet">
        </a>
    </li>
    <li>Nov 7, 2023: Our paper, <a href="https://arxiv.org/pdf/2311.01460.pdf">Implicit Chain of Thought Reasoning via Knowledge Distillation</a>, is now publicly available! This paper trains LMs that can reason internally using hidden states instead of articulating all reasoning steps like humans.
        <a class="twitter-link" href="https://twitter.com/yuntiandeng/status/1721934898582229498">
            <img src="images/twitter_logo.svg" alt="Tweet">
        </a>
    </li>
    <li>Mar 29, 2023: <a href="https://openaiwatch.com/">OpenAIWatch.com</a> is launched! It tracks GPT-4's nondeterministic behavior even with greedy decoding in unicorn illustrations. 🦄 
        <a class="twitter-link" href="https://twitter.com/yuntiandeng/status/1641108596510343168">
            <img src="images/twitter_logo.svg" alt="Tweet">
        </a>
    </li>
    <li>Mar 29, 2023: Our <a href="https://huggingface.co/spaces/yuntian-deng/ChatGPT">GPT Chatbot</a>, based on Yuvraj Sharma's <a href="https://huggingface.co/spaces/ysharma/ChatGPT4">code</a>, is now live! It provides free acess to GPT with the aim of collecting dialogue data for research purposes.
    </li>
    <li>Oct 18, 2022: Our paper, <a href="https://aclanthology.org/2022.emnlp-main.815.pdf">Model Criticism for Long-Form Text Generation</a>, is now publicly available! This paper uses model criticism in latent space to quantify various notions of high-level coherence in long-form text generation.
        <a class="twitter-link" href="https://twitter.com/srush_nlp/status/1582410985226108930">
            <img src="images/twitter_logo.svg" alt="Tweet">
        </a>
    </li>
    <li>Oct 12, 2022: <a href="https://huggingface.co/spaces/yuntian-deng/latex2im">Markup-to-Image Diffusion Models demo</a> is now live! This project uses a diffusion model to learn how to render various types of markups, including LaTeX.
        <a class="twitter-link" href="https://twitter.com/srush_nlp/status/1580255172340879360">
            <img src="images/twitter_logo.svg" alt="Tweet">
        </a>
    </li>
    <li>Jun 2, 2020: Our paper, <a href="https://papers.nips.cc/paper/2020/file/01a0683665f38d8e5e567b3b15ca98bf-Paper.pdf">Cascaded Text Generation with Markov Transformers</a>, is available! It allows parallel, fast, autoregressive, and accurate text generation using a high-order Markov model.
        <a class="twitter-link" href="https://twitter.com/srush_nlp/status/1267866675904417801">
            <img src="images/twitter_logo.svg" alt="Tweet">
        </a>
    </li>
    <li>Apr 26, 2020: Introducing <a href="https://openreview.net/pdf?id=B1l4SgHKDH">Residual Energy-Based Models for Text Generation</a>, a globally-normalized approach to text generation! Our approach uses a global discriminator to guide the traditional locally-normalized language model to produce text that's more indistinguishable from human-written text.
    </li>
    <li>Sep 5, 2019: <a href="https://steganography.live">Neural Linguistic Steganography demo</a> is now live! This project lets you hide secret messages in natural language using arithmetic coding.
        <a class="twitter-link" href="https://twitter.com/srush_nlp/status/1169647490284605443">
            <img src="images/twitter_logo.svg" alt="Tweet">
        </a>
    </li>
    <li>Dec 19, 2016: Excited to introduce <a href="http://opennmt.net">OpenNMT</a>, an open-source neural machine translation toolkit developed for industrial and academic use.
        <a class="twitter-link" href="https://twitter.com/srush_nlp/status/810900018907533313">
            <img src="images/twitter_logo.svg" alt="Tweet">
        </a>
    </li>
    <li>Sep 19, 2016: Excited to announce that we've provided a solution to OpenAI's <a href="https://github.com/openai/requests-for-research/blob/master/_requests_for_research/im2latex.html">requests-for-research im2latex challenge</a> using neural sequence-to-sequence learning! Check out the visualizations <a href="https://im2markup.yuntiandeng.com">here</a>.
        <a class="twitter-link" href="https://twitter.com/srush_nlp/status/777901557585178624">
            <img src="images/twitter_logo.svg" alt="Tweet">
        </a>
    </li>
</ul>
<button id="toggle-news-btn" onclick="toggleNews()">Hide Older News</button>

<!--h3> Current Research Area </h3>
<p style="font-size: 18px">
<ul style="font-size: 18px">
  <li> Deep generative models for probabilistic text generation. </li>
</ul>
</p-->

<br>
<h4>Representative Works</h4>
<p style="font-size: 18px">These are some of my representative works. For all my papers, visit <a href="/papers">here</a> or my <a href="https://scholar.google.com/citations?user=tk0e5lYAAAAJ">Google Scholar</a>.</p>
<table>
{% for paper in site.data.papers.notable %}

  <tr><td style="padding:10px">
      {% if paper.image %}
      <a href="{{paper.pdf}}"><img width="300" style="min-width:100px" src="{{paper.image}}" alt=""></a>
      {% else %}
      <a href="{{paper.pdf}}"><img height="75" style="min-width:100px" src="https://avatar.tobi.sh/{{paper.title}}" alt=""></a>

      {% endif %}
</td><td style="padding:10px; font-size: 16px">
<a class="paper" href="{{paper.pdf}}">
{{paper.title}}
</a><br>
{{paper.authors}}.<br>
{{paper.conference}} <br>
{% if paper.notes %}
{{paper.notes}}<br>
{% else %}
{% endif %}

<br>

</td></tr>

{% endfor %}
</table>

<br>
<!--h4>Prospective Students</h4-->
<!--p style="font-size: 16px">
I plan to take new PhD students this year. Special consideration will be given to those who can solve the following challenge by January 2025: In the GPT2 model trained to directly solve 20-by-20 multiplication without using intermediate steps (implicit chain-of-thought by learning to internalize step by step), what is the internal algorithm that the trained model uses to solve multiplication directly? (This is not about the training procedure. but about the model's internal reasoning process post-training.) Please see <a href="https://huggingface.co/spaces/yuntian-deng/gpt2-multiplication">our demo</a> and <a href="https://arxiv.org/abs/2405.14838">our paper</a> for more details.
</p-->
<!--p style="font-size: 16px">
<strong>Important Note:</strong> I kindly request prospective students to apply directly through the University of Waterloo's application system. Please refrain from sending me emails without a <strong>specific</strong> question.
</p-->

<!--h4>Contact</h4>

<div class="paper-section">
    Science and Engineering Complex 5.443 <br>
    150 Western Avenue <br>
    Boston, MA 02134 <br>
    USA
</div-->
<br>
